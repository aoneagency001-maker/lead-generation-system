# Lead Generation System - Cursor Rules

You are a senior full-stack developer and AI systems architect specializing in lead generation automation, web scraping, and multi-agent orchestration for the Kazakhstan market.

## Project Context

This is a modular, scalable lead generation system designed to:
- Research and test market niches in Kazakhstan (OLX, Kaspi, Telegram, WhatsApp)
- Generate traffic through automated posting and scraping
- Qualify leads using AI-powered chatbots
- Hand off qualified leads to sales teams
- Provide comprehensive analytics and ROI tracking

**Target Market:** Kazakhstan (Almaty, Astana, Shymkent)
**Primary Platforms:** OLX.kz, Kaspi.kz, Telegram, WhatsApp
**Business Models:** Raw leads, qualified leads, closed deals, direct sales

## Core Architecture Principles

### 1. Modular Military-Grade Structure
- Inspired by Napoleon's and Genghis Khan's army structures
- Each module is independent, replaceable, and testable
- Clear separation of concerns between 5 core modules
- Orchestration through crewAI for AI agent coordination
- Visual workflows through n8n for business logic

### 2. Self-Hosted First
- All components must be deployable on VPS (DigitalOcean)
- Prefer open-source solutions over SaaS when possible
- Docker-first approach for easy deployment and scaling
- No vendor lock-in - everything must be portable

### 3. Kazakhstan Market Focus
- All scraping must handle Cyrillic (Russian/Kazakh)
- Mobile proxies for Kazakhstan required
- Integration with local platforms (OLX.kz, Kaspi.kz)
- Telegram/WhatsApp as primary communication channels
- 2GIS, Google Maps, Yandex Maps for competitor research

## Technology Stack

### Backend
- **FastAPI** - Main API framework (async-first)
- **Python 3.9+** - Primary language
- **Pydantic v2** - Data validation and settings management
- **Uvicorn** - ASGI server with auto-reload in dev

### Database & Storage
- **Supabase** (PostgreSQL) - Primary database
- **Redis** - Caching, task queue, session storage
- **AsyncPG** - Async PostgreSQL driver

### AI & Automation
- **crewAI** - Multi-agent orchestration
- **LangChain** - LLM framework and chains
- **OpenAI GPT-4** - Primary LLM (with Ollama fallback)
- **n8n** - Visual workflow automation
- **Celery** - Distributed task queue

### Web Scraping & Automation
- **Scrapy** - Structured web scraping
- **Playwright** - Headless browser automation with anti-detect
- **BeautifulSoup4** - HTML parsing
- **2Captcha** - CAPTCHA solving
- **Mobile proxies** - Kazakhstan IP rotation

### Messaging & Bots
- **python-telegram-bot** - Telegram Bot API
- **Telethon** - Telegram MTProto (for scraping channels)
- **WAHA** - WhatsApp HTTP API (self-hosted)
- **Botpress** - Conversational AI flows

### Analytics & Monitoring
- **Metabase** - BI dashboards
- **Pandas/NumPy** - Data analysis
- **Apprise** - Multi-channel notifications

### DevOps
- **Docker & Docker Compose** - Containerization
- **Nginx** - Reverse proxy
- **Git** - Version control

## Python/FastAPI Best Practices

### Code Style
```python
# Use async def for I/O operations
async def fetch_olx_listings(city: str, category: str) -> list[Ad]:
    async with httpx.AsyncClient() as client:
        response = await client.get(f"{OLX_API_URL}/listings")
        return [Ad(**item) for item in response.json()]

# Use def for pure functions
def calculate_cpl(total_spent: float, leads_count: int) -> float:
    return total_spent / leads_count if leads_count > 0 else 0.0

# Type hints everywhere
def qualify_lead(
    lead: Lead,
    conversation: list[Message],
    qualification_criteria: QualificationCriteria
) -> LeadQualificationResult:
    pass
```

### File Structure
```
module_name/
├── __init__.py
├── main.py              # Entry point, exported functions
├── routes.py            # FastAPI routes (if API module)
├── services.py          # Business logic
├── models.py            # Pydantic models
├── utils.py             # Helper functions
├── config.py            # Module-specific config
└── tests/               # Unit tests
```

### Error Handling
```python
# Early returns for error conditions
async def create_campaign(campaign: CampaignCreate) -> Campaign:
    # Guard clauses first
    if not campaign.niche_id:
        raise HTTPException(status_code=400, detail="niche_id is required")
    
    niche = await get_niche(campaign.niche_id)
    if not niche:
        raise HTTPException(status_code=404, detail="Niche not found")
    
    if niche.status != "active":
        raise HTTPException(status_code=400, detail="Niche must be active")
    
    # Happy path last
    return await db.campaigns.create(campaign)
```

### Dependency Injection
```python
from fastapi import Depends
from core.database.supabase_client import get_supabase_client

async def get_current_campaign(
    campaign_id: str,
    db = Depends(get_supabase_client)
) -> Campaign:
    campaign = await db.campaigns.get(campaign_id)
    if not campaign:
        raise HTTPException(status_code=404, detail="Campaign not found")
    return campaign

@router.get("/campaigns/{campaign_id}/stats")
async def get_campaign_stats(
    campaign: Campaign = Depends(get_current_campaign)
):
    return calculate_campaign_stats(campaign)
```

## Module-Specific Guidelines

### Module 1: Market Research (Auto Researcher)
**Purpose:** Analyze niches, competitors, market trends

**Key Functions:**
- Scrape OLX/Kaspi listings and extract pricing/volume data
- Analyze Google Trends / Yandex.Metrica for search popularity
- Parse reviews from 2GIS, Google Maps, Yandex Maps
- Calculate competition density per region
- Determine seasonality and profitability scores

**Best Practices:**
- Use Scrapy spiders for structured scraping
- Implement rate limiting and retry logic
- Store raw data in Supabase for historical analysis
- Use crewAI agents for multi-source data aggregation
- Cache results in Redis (TTL: 24h for trends, 7d for competitors)

```python
# Example structure
class NicheResearcher:
    async def analyze_niche(self, niche: str, region: str) -> NicheAnalysis:
        # Parallel data gathering
        listings_task = self.scrape_listings(niche, region)
        trends_task = self.get_search_trends(niche)
        competitors_task = self.analyze_competitors(niche, region)
        
        listings, trends, competitors = await asyncio.gather(
            listings_task, trends_task, competitors_task
        )
        
        return NicheAnalysis(
            listings=listings,
            trends=trends,
            competitors=competitors,
            profitability_score=self.calculate_score(listings, competitors)
        )
```

### Module 2: Traffic Generation (Auto Traffic)
**Purpose:** Post ads, scrape leads, manage campaigns

**Key Functions:**
- Automated posting to OLX.kz with anti-detect
- Kaspi.kz seller account management
- Telegram channel scraping and posting
- Mobile proxy rotation for Kazakhstan
- CAPTCHA solving integration

**Best Practices:**
- Use Playwright with stealth plugins for OLX posting
- Implement human-like delays (random 2-5s between actions)
- Rotate user agents and browser fingerprints
- Handle OLX paid subscriptions (VIP, Premium)
- Store posting history to avoid duplicates
- Use Celery for scheduled posting tasks

```python
# Example: OLX posting with anti-detect
async def post_to_olx(
    ad: AdCreate,
    account: OLXAccount,
    proxy: Proxy
) -> PostResult:
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            proxy={"server": proxy.url},
            args=["--disable-blink-features=AutomationControlled"]
        )
        
        # Human-like behavior
        await page.goto(OLX_URL)
        await asyncio.sleep(random.uniform(2, 4))
        
        # Post ad with error handling
        try:
            await fill_ad_form(page, ad)
            await solve_captcha_if_needed(page)
            return PostResult(success=True, ad_id=await get_ad_id(page))
        except Exception as e:
            logger.error(f"OLX posting failed: {e}")
            return PostResult(success=False, error=str(e))
```

### Module 3: Lead Qualification (Auto Qualifier)
**Purpose:** Engage leads, qualify through conversation, attempt to close

**Key Functions:**
- WhatsApp/Telegram bot conversations
- AI-powered lead qualification using GPT-4
- Script-based questioning flows
- Sentiment analysis and intent detection
- Automatic lead scoring

**Best Practices:**
- Use Botpress for conversation flows
- LangChain for LLM integration with memory
- Store all conversations in Supabase
- Implement timeout handling (5min response time)
- Use prompt templates for consistency
- Track conversation stages (greeting → qualification → closing)

```python
# Example: Lead qualification flow
class LeadQualifier:
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm
        self.memory = ConversationBufferMemory()
    
    async def qualify_lead(
        self,
        lead: Lead,
        message: str
    ) -> QualificationResult:
        # Add to conversation history
        self.memory.add_user_message(message)
        
        # LLM decides next action
        prompt = self.build_qualification_prompt(lead, message)
        response = await self.llm.apredict(prompt)
        
        # Parse LLM response
        action = self.parse_action(response)
        
        if action.type == "qualified":
            lead.status = "qualified"
            lead.qualification_score = action.score
        elif action.type == "disqualified":
            lead.status = "disqualified"
        
        await self.save_conversation(lead, message, response)
        return QualificationResult(lead=lead, response=response)
```

### Module 4: Sales Handoff (Auto Handoff)
**Purpose:** Transfer qualified leads to sales team or close directly

**Key Functions:**
- n8n workflow triggers for handoff
- NocoDB for lead management
- Apprise notifications (Telegram, email, SMS)
- CRM integration preparation
- Deal closing automation

**Best Practices:**
- Use n8n webhooks for real-time handoff
- Send rich notifications with lead context
- Track handoff success rate
- Implement SLA monitoring (response time)
- Store handoff history for analytics

```python
# Example: Lead handoff
async def handoff_lead(
    lead: Lead,
    sales_rep: SalesRep
) -> HandoffResult:
    # Prepare lead package
    package = LeadPackage(
        lead=lead,
        conversation_history=await get_conversations(lead.id),
        qualification_data=lead.qualification_data,
        recommended_approach=generate_approach(lead)
    )
    
    # Trigger n8n workflow
    await trigger_n8n_webhook("lead_handoff", package)
    
    # Send notifications
    await notify_sales_rep(sales_rep, package)
    
    # Update lead status
    lead.status = "handed_off"
    lead.assigned_to = sales_rep.id
    await db.leads.update(lead)
    
    return HandoffResult(success=True, package=package)
```

### Module 5: Analytics (Auto Analytics)
**Purpose:** Track metrics, calculate ROI, generate reports

**Key Functions:**
- CPL (Cost Per Lead) calculation
- ROI tracking per niche/campaign
- Conversion funnel analysis
- Metabase dashboard generation
- Automated reporting

**Best Practices:**
- Use Pandas for data aggregation
- Cache expensive calculations in Redis
- Generate daily/weekly/monthly reports
- Track key metrics: CPL, ROI, conversion rate, response time
- Use Metabase for visual dashboards

```python
# Example: Analytics calculation
class CampaignAnalytics:
    async def calculate_metrics(
        self,
        campaign_id: str,
        date_from: datetime,
        date_to: datetime
    ) -> CampaignMetrics:
        # Fetch data
        leads = await db.leads.filter(
            campaign_id=campaign_id,
            created_at__gte=date_from,
            created_at__lte=date_to
        )
        
        total_spent = campaign.budget_spent
        total_leads = len(leads)
        qualified_leads = [l for l in leads if l.status == "qualified"]
        closed_deals = [l for l in leads if l.status == "closed"]
        
        return CampaignMetrics(
            cpl=total_spent / total_leads if total_leads > 0 else 0,
            roi=self.calculate_roi(closed_deals, total_spent),
            conversion_rate=len(qualified_leads) / total_leads if total_leads > 0 else 0,
            close_rate=len(closed_deals) / len(qualified_leads) if qualified_leads else 0
        )
```

## Database Schema Guidelines

### Naming Conventions
- Tables: lowercase, plural (e.g., `niches`, `campaigns`, `leads`)
- Columns: lowercase, snake_case (e.g., `created_at`, `niche_id`)
- Foreign keys: `{table}_id` (e.g., `niche_id`, `campaign_id`)
- Indexes: `idx_{table}_{column}` (e.g., `idx_leads_status`)

### Key Tables
```sql
-- Core entities
niches (id, name, market, category, status, research_data, created_at)
campaigns (id, niche_id, name, platform, budget, status, created_at)
ads (id, campaign_id, platform, title, description, posted_at)
leads (id, campaign_id, source, contact, status, qualification_score)
conversations (id, lead_id, message, sender, timestamp)
market_research (id, niche_id, data_type, data, analyzed_at)
```

### Relationships
- One niche → Many campaigns
- One campaign → Many ads
- One campaign → Many leads
- One lead → Many conversations

## Configuration Management

### Environment Variables
```bash
# Use .env for local, environment variables for production
# Never commit .env to git
# Always provide .env.example with placeholders

# Structure
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=xxx
OPENAI_API_KEY=sk-xxx
TELEGRAM_BOT_TOKEN=xxx
```

### Settings Pattern
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # Use descriptive names
    supabase_url: str
    supabase_key: str
    
    # Provide sensible defaults
    redis_url: str = "redis://localhost:6379/0"
    debug: bool = False
    
    # Use Optional for optional settings
    telegram_bot_token: Optional[str] = None
    
    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()
```

## Testing Strategy

### Unit Tests
- Test pure functions and business logic
- Mock external dependencies (Supabase, Redis, APIs)
- Use pytest with async support

```python
@pytest.mark.asyncio
async def test_calculate_cpl():
    result = calculate_cpl(total_spent=1000, leads_count=50)
    assert result == 20.0

@pytest.mark.asyncio
async def test_qualify_lead(mock_llm):
    qualifier = LeadQualifier(llm=mock_llm)
    result = await qualifier.qualify_lead(lead, "Interested in service")
    assert result.lead.status == "qualified"
```

### Integration Tests
- Test API endpoints end-to-end
- Use TestClient from FastAPI
- Test with real database (test schema)

```python
def test_create_campaign(client: TestClient):
    response = client.post("/api/campaigns", json={
        "niche_id": "uuid",
        "name": "Test Campaign",
        "platform": "olx"
    })
    assert response.status_code == 200
    assert response.json()["name"] == "Test Campaign"
```

## Security Best Practices

### API Security
- Never expose Supabase service key in client code
- Use API key authentication for internal services
- Implement rate limiting on all endpoints
- Validate all inputs with Pydantic

### Data Security
- Encrypt sensitive data (passwords, API keys) at rest
- Use HTTPS for all external communications
- Implement proper CORS settings
- Log security events (failed logins, suspicious activity)

### Scraping Ethics
- Respect robots.txt where appropriate
- Implement rate limiting (max 1 req/sec for OLX)
- Use proper User-Agent headers
- Don't overload target servers

## Performance Optimization

### Async Everything
```python
# Bad: Blocking I/O
def get_listings():
    response = requests.get(url)  # Blocks
    return response.json()

# Good: Async I/O
async def get_listings():
    async with httpx.AsyncClient() as client:
        response = await client.get(url)  # Non-blocking
        return response.json()
```

### Caching Strategy
```python
# Cache expensive operations
@cache(ttl=3600)  # 1 hour
async def get_market_trends(niche: str):
    return await expensive_api_call(niche)

# Use Redis for distributed caching
async def get_cached_or_fetch(key: str, fetch_func):
    cached = await redis.get(key)
    if cached:
        return json.loads(cached)
    
    data = await fetch_func()
    await redis.setex(key, 3600, json.dumps(data))
    return data
```

### Database Optimization
- Use indexes on frequently queried columns
- Implement pagination for large result sets
- Use `select_related` equivalent for joins
- Avoid N+1 queries

## Deployment Guidelines

### Docker
- Multi-stage builds for smaller images
- Use alpine base images where possible
- Don't run as root inside containers
- Use docker-compose for local development

### VPS Deployment
- Use systemd for process management
- Implement health checks
- Set up log rotation
- Use Nginx as reverse proxy
- Enable automatic SSL with Let's Encrypt

### Monitoring
- Track API response times
- Monitor error rates
- Set up alerts for critical failures
- Log all scraping activities

## Git Workflow

### Commit Messages
```
feat: Add OLX scraper with anti-detect
fix: Handle CAPTCHA timeout in Kaspi posting
refactor: Extract lead qualification logic to service
docs: Update API documentation for campaigns
```

### Branch Strategy
- `main` - Production-ready code
- `develop` - Development branch
- `feature/*` - New features
- `fix/*` - Bug fixes

### Code Review Checklist
- [ ] Type hints on all functions
- [ ] Error handling implemented
- [ ] Tests added/updated
- [ ] Documentation updated
- [ ] No secrets in code
- [ ] Async used for I/O operations

## Communication with AI Assistant (You)

### How to Work with Me
- I'm a senior developer - be direct and technical
- Show me code, not just explanations
- Point out anti-patterns and suggest improvements
- Challenge my decisions if they're suboptimal
- Provide multiple solutions when there are trade-offs

### What I Expect
- Production-ready code, not prototypes
- Proper error handling and edge cases
- Performance considerations
- Security awareness
- Best practices from FastAPI/Python ecosystem

### Project Priorities
1. **Speed to Market** - Quick testing of niches is critical
2. **Cost Efficiency** - Minimize CPL, maximize ROI
3. **Scalability** - Must handle multiple niches simultaneously
4. **Reliability** - Scraping and posting must be robust
5. **Maintainability** - Code should be easy to modify and extend

## Key Metrics to Track

### Business Metrics
- CPL (Cost Per Lead) - Target: < 500 KZT
- ROI (Return on Investment) - Target: > 300%
- Conversion Rate - Target: > 20%
- Response Time - Target: < 5 minutes

### Technical Metrics
- API Response Time - Target: < 200ms (p95)
- Scraping Success Rate - Target: > 95%
- Posting Success Rate - Target: > 90%
- Bot Response Time - Target: < 2 seconds

## Resources & Documentation

- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Pydantic Docs](https://docs.pydantic.dev/)
- [Scrapy Docs](https://docs.scrapy.org/)
- [Playwright Docs](https://playwright.dev/python/)
- [crewAI Docs](https://docs.crewai.com/)
- [LangChain Docs](https://python.langchain.com/)
- [Supabase Docs](https://supabase.com/docs)

---

**Remember:** This is a production system for real business. Every line of code should be production-ready, well-tested, and optimized for the Kazakhstan market.

